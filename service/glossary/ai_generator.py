# lightVT/service/glossary/ai_generator.py - å†…éƒ¨å®ç°ç¿»è¯‘å‡½æ•°

import json
import re
import math
import threading
from typing import Dict, List, Set, Tuple, Optional, Callable
from dataclasses import dataclass
from service import log,localization
from llama_cpp import Llama

logger = log.get_logger("AIGlossaryGenerator")
_progress_var = 0.0

@dataclass
class ExtractionConfig:
    """æœ¯è¯­æå–é…ç½®"""
    chunk_size: int = 200
    chunk_overlap: int = 50
    min_term_frequency: int = 2
    max_terms_per_chunk: int = 15
    min_term_length: int = 2
    max_term_length: int = 50

def _create_chat_completion(prompt: str,  llm: Llama) -> str:
    """ğŸ”¥ å†…éƒ¨ç¿»è¯‘å‡½æ•°"""
    try:
        response = llm.create_chat_completion(
            messages=[
                {"role": "user", "content": prompt}
            ]
            ,
            temperature=0.1,
            max_tokens=4096,
        )
        return response["choices"][0]["message"]["content"].strip()
    except Exception as e:
        logger.error(f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {e}")
        raise e

def generate_glossary_from_subtitle(
    subtitle_text: str, 
    target_language: str,
    model_path: str,
    config: Optional[ExtractionConfig],
    stop_event: Optional[threading.Event],
    n_gpu_layers: int = -1,
    update_progress: Callable[[str, float], None] = None
) -> Dict[str, str]:
    """ğŸ”¥ ä»å­—å¹•æ–‡æœ¬ç”Ÿæˆæœ¯è¯­è¡¨ - ä¸»å‡½æ•°ï¼ˆç§»é™¤å¤–éƒ¨ç¿»è¯‘å‡½æ•°å‚æ•°ï¼‰"""
    global _progress_var
    
    if not config:
        config = ExtractionConfig()
    
    logger.info("å¼€å§‹ä»å­—å¹•æ–‡æœ¬ç”Ÿæˆæœ¯è¯­è¡¨")
    
    try:
        def add_progress(msg:str,increment: float):
            global _progress_var
            if update_progress:
                _progress_var += increment
                update_progress(msg, _progress_var)
                
        def set_progress(msg: str, value: float):
            global _progress_var
            if update_progress:
                _progress_var = value
                update_progress(msg, _progress_var)

        if stop_event.is_set():
            logger.info("ç”Ÿæˆæœ¯è¯­è¡¨å·²è¢«å–æ¶ˆ")
            return {}
        
        set_progress('', 0.0)

        # æ­¥éª¤1: é¢„å¤„ç†å’Œåˆ‡ç‰‡
        cleaned_text = clean_subtitle_text(subtitle_text)
        chunks = split_text_into_chunks(cleaned_text, config)
        if not chunks:
            logger.warning("æœªæå–åˆ°æœ‰æ•ˆæ–‡æœ¬ç‰‡æ®µ")
            return {}
        logger.info(f"æ–‡æœ¬å·²åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µ")
        add_progress(localization.get("log_glossary_text_split").format(count=len(chunks)), 0.1)

        if stop_event.is_set():
            logger.info("ç”Ÿæˆæœ¯è¯­è¡¨å·²è¢«å–æ¶ˆ")
            return {}
        
        llm = Llama(
            model_path=model_path,
            n_gpu_layers=n_gpu_layers,
            n_ctx=4096
        )
        # æ­¥éª¤2: ä»æ¯ä¸ªåˆ‡ç‰‡æå–æœ¯è¯­ï¼ˆåŒ…å«ä¸Šä¸‹æ–‡ï¼‰
        term_contexts = extract_terms_with_context(chunks, llm, config, stop_event, add_progress)
        logger.info(f"æå–åˆ° {len(term_contexts)} ä¸ªå¸¦ä¸Šä¸‹æ–‡çš„æœ¯è¯­")
        logger.info(f"æœ¯è¯­åŠä¸Šä¸‹æ–‡: {term_contexts}")
        
        # æ­¥éª¤3: ç»Ÿè®¡æœ¯è¯­é¢‘ç‡
        term_frequencies = calculate_term_frequencies(term_contexts, cleaned_text)
        add_progress(localization.get("log_glossary_term_frequency_completed"), 0.05)
        logger.info(f"æœ¯è¯­é¢‘ç‡è¡¨ï¼š{term_frequencies}")

        # æ­¥éª¤4: è¿‡æ»¤é«˜é¢‘ç‡æœ¯è¯­
        high_freq_terms = filter_high_frequency_terms(term_frequencies, config)
        add_progress(localization.get("log_glossary_filter_terms").format(count=len(high_freq_terms)), 0.05)

        if not high_freq_terms:
            logger.warning("æœªæå–åˆ°æœ‰æ•ˆæœ¯è¯­")
            return {}
        
        # æ­¥éª¤5: å¸¦ä¸Šä¸‹æ–‡çš„æœ¯è¯­ç¿»è¯‘
        glossary = translate_terms_with_context(high_freq_terms, term_contexts, target_language, llm, stop_event, add_progress)

        logger.info(f"æœ¯è¯­è¡¨ç”Ÿæˆå®Œæˆï¼Œå…± {len(glossary)} ä¸ªæœ¯è¯­å¯¹")
        
        set_progress(localization.get('completed'), 1.0)
        
        return glossary
        
    except Exception as e:
        logger.error(f"ç”Ÿæˆæœ¯è¯­è¡¨å¤±è´¥: {e}")
        return {}

def clean_subtitle_text(text: str) -> str:
    """æ¸…ç†å­—å¹•æ–‡æœ¬"""
    # ç§»é™¤æ—¶é—´æˆ³
    text = re.sub(r'\d{2}:\d{2}:\d{2}[.,]\d{3}\s*-->\s*\d{2}:\d{2}:\d{2}[.,]\d{3}', '', text)
    
    # ç§»é™¤åºå·è¡Œ
    text = re.sub(r'^\d+\s*$', '', text, flags=re.MULTILINE)
    
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # ç§»é™¤å­—å¹•æ ¼å¼æ ‡è®°
    text = re.sub(r'\{[^}]*\}', '', text)  # ç§»é™¤ {font} ç­‰æ ‡è®°
    text = re.sub(r'\[[^\]]*\]', '', text)  # ç§»é™¤ [music] ç­‰æ ‡è®°
    
    # ç»Ÿä¸€æ¢è¡Œå’Œç©ºæ ¼
    text = re.sub(r'\n\s*\n', '\n', text)
    text = re.sub(r'[ \t]+', ' ', text)
    
    return text.strip()

def split_text_into_chunks(text: str, config: ExtractionConfig) -> List[str]:
    """å°†æ–‡æœ¬åˆ‡åˆ†ä¸ºå¸¦é‡å çš„ç‰‡æ®µ - ç®€åŒ–å®‰å…¨ç‰ˆ"""
    if len(text) <= config.chunk_size:
        return [text]
    
    chunks = []
    start = 0
    
    while start < len(text):
        # è®¡ç®—ç‰‡æ®µç»“æŸä½ç½®
        end = min(start + config.chunk_size, len(text))
        
        # åœ¨å¥å­è¾¹ç•Œåˆ‡åˆ†ï¼ˆé™åˆ¶å›é€€èŒƒå›´ï¼‰
        if end < len(text):
            # åªåœ¨å25%èŒƒå›´å†…å¯»æ‰¾å¥å­è¾¹ç•Œ
            search_start = max(start + config.chunk_size * 3 // 4, start)
            for i in range(end, search_start, -1):
                if text[i] in '.!?ã€‚ï¼ï¼Ÿ\n':
                    end = i + 1
                    break
        
        # æå–ç‰‡æ®µ
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # å¦‚æœåˆ°è¾¾æœ«å°¾ï¼Œé€€å‡º
        if end >= len(text):
            break
        
        # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·å§‹ä½ç½®ï¼Œç¡®ä¿å‰è¿›
        start = max(start + 1, end - config.chunk_overlap)
    
    return chunks

def extract_terms_with_context(
    chunks: List[str], 
    llm: Llama,
    config: ExtractionConfig,
    stop_event: threading.Event,
    add_progress: Callable
) -> Dict[str, List[str]]:
    """ğŸ”¥ ä»æ–‡æœ¬ç‰‡æ®µæå–æœ¯è¯­åŠå…¶ä¸Šä¸‹æ–‡ï¼ˆç§»é™¤å¤–éƒ¨ç¿»è¯‘å‡½æ•°å‚æ•°ï¼‰"""
    global _progress_var
    if chunks is None or len(chunks) == 0:
        logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ–‡æœ¬ç‰‡æ®µ")
        return {}
    
    term_contexts: Dict[str, List[str]] = {}  # æœ¯è¯­ -> ä¸Šä¸‹æ–‡åˆ—è¡¨
    
    progress_step = 0.5 / len(chunks)
    
    for i, chunk in enumerate(chunks):
        if stop_event.is_set():
            logger.info("æœ¯è¯­æå–å·²è¢«å–æ¶ˆ")
            return {}
        
        logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(chunks)} ä¸ªç‰‡æ®µ...")
        
        try:
            # æå–æœ¯è¯­æ—¶ä¿ç•™ä¸Šä¸‹æ–‡ä¿¡æ¯
            chunk_terms = extract_terms_from_chunk(chunk, config, i+1, llm)
            
            # ä¸ºæ¯ä¸ªæœ¯è¯­è®°å½•ä¸Šä¸‹æ–‡
            for term in chunk_terms:
                if term not in term_contexts:
                    term_contexts[term] = []
                
                # æå–æœ¯è¯­çš„ä¸Šä¸‹æ–‡ï¼ˆå‰åå„50ä¸ªå­—ç¬¦ï¼‰
                context = extract_term_context(chunk, term, context_window=80)
                if context and context not in term_contexts[term]:
                    term_contexts[term].append(context)
            
            logger.debug(f"æå–åˆ°çš„æœ¯è¯­: {chunk_terms}")

            add_progress(localization.get("log_glossary_chunk_complete").format(chunk_index=i+1, chunk_count=len(chunks), term_count=len(chunk_terms)), progress_step)

        except Exception as e:
            logger.error(f"å¤„ç†ç‰‡æ®µ {i+1} å¤±è´¥: {e}")
            continue
    
    return term_contexts

def extract_terms_from_chunk(
    chunk: str, 
    config: ExtractionConfig, 
    chunk_index: int,
    llm: Llama
) -> List[str]:
    """ğŸ”¥ ä»å•ä¸ªæ–‡æœ¬ç‰‡æ®µæå–æœ¯è¯­ï¼ˆä½¿ç”¨å†…éƒ¨ç¿»è¯‘å‡½æ•°ï¼‰"""
    
    # æ”¹è¿›çš„æå–æç¤ºï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡æŒ‡å¯¼
    prompt = f"""
è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–æ²¡æœ‰é€šç”¨ç¿»è¯‘æ ‡å‡†çš„ä¸“æœ‰åè¯ã€‚

ã€æå–æ ‡å‡†ã€‘
1. **äººå**
2. **ç»°å·/æ˜µç§°**
3. **å…·ä½“åœ°å**
4. **æœºæ„åœ°å**

ã€é¿å…æå–ã€‘
- å¸¸è§åŠ¨è¯ã€å½¢å®¹è¯ã€å‰¯è¯ã€ä»‹è¯ã€è¿è¯
- ä¸€èˆ¬æ€§ä¸“ä¸šè¯æ±‡ï¼ˆå¦‚ï¼šcomputer, internet, music, art, scienceç­‰ï¼‰
- å¸¸è§éŸ³ä¹æœ¯è¯­
- å¸¸è§ä½“è‚²æœ¯è¯­
- åŸºç¡€å­¦ç§‘è¯æ±‡
- æœ‰é€šç”¨ç¿»è¯‘æ ‡å‡†çš„å¤§ä¼—åŒ–çš„è¯æ±‡

ã€è´¨é‡è¦æ±‚ã€‘
- æœ¯è¯­é•¿åº¦åœ¨{config.min_term_length}-{config.max_term_length}ä¸ªå­—ç¬¦
- æœ€å¤šæå–{config.max_terms_per_chunk}ä¸ªæœ€é‡è¦çš„æœ¯è¯­

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¾“å‡ºï¼š
["term1", "term2","term3", ...]

ã€æ–‡æœ¬å†…å®¹ã€‘
{chunk}

è¯·å¼€å§‹æå–ï¼š
"""
    try:
        # ğŸ”¥ ä½¿ç”¨å†…éƒ¨ç¿»è¯‘å‡½æ•°
        response = _create_chat_completion(prompt, llm)
        terms = parse_term_extraction_response(response)
        return terms
        
    except Exception as e:
        logger.error(f"ç‰‡æ®µ {chunk_index} æœ¯è¯­æå–å¤±è´¥: {e}")
        return []

def parse_term_extraction_response(response: str) -> List[str]:
    """è§£ææœ¯è¯­æå–å“åº”"""
    try:
        # æå–JSONæ•°ç»„
        json_match = re.search(r'\[.*?\]', response, re.DOTALL)
        if json_match:
            json_text = json_match.group()
            terms_list = json.loads(json_text)
            
            if isinstance(terms_list, list):
                return [term.strip().lower() for term in terms_list if isinstance(term, str) and term.strip()]
        
        # å¤‡ç”¨è§£æï¼šé€è¡Œæå–
        terms = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('"') and line.endswith('"'):
                term = line.strip('"').strip().lower()
                if 2 <= len(term) <= 50:
                    terms.append(term)
        
        return terms[:15]  # é™åˆ¶æ•°é‡
        
    except Exception as e:
        logger.error(f"è§£ææœ¯è¯­æå–å“åº”å¤±è´¥: {e}")
        return []

def extract_term_context(text: str, term: str, context_window: int = 50) -> str:
    """æå–æœ¯è¯­çš„ä¸Šä¸‹æ–‡"""
    try:
        # æŸ¥æ‰¾æœ¯è¯­ä½ç½®ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
        pattern = re.compile(re.escape(term), re.IGNORECASE)
        match = pattern.search(text)
        
        if not match:
            return ""
        
        start_pos = match.start()
        end_pos = match.end()
        
        # æå–å‰åä¸Šä¸‹æ–‡
        context_start = max(0, start_pos - context_window)
        context_end = min(len(text), end_pos + context_window)
        
        context = text[context_start:context_end].strip()
        
        # åœ¨è¾¹ç•Œå¤„æˆªæ–­åˆ°å®Œæ•´å•è¯
        if context_start > 0:
            space_pos = context.find(' ')
            if space_pos > 0:
                context = context[space_pos+1:]
        
        if context_end < len(text):
            space_pos = context.rfind(' ')
            if space_pos > 0:
                context = context[:space_pos]
        
        return context
        
    except Exception:
        return ""

def calculate_term_frequencies(term_contexts: Dict[str, List[str]], cleaned_text: str) -> Dict[str, int]:
    """è®¡ç®—æœ¯è¯­é¢‘ç‡"""
    return {term: cleaned_text.lower().count(term) for term, contexts in term_contexts.items()}

def filter_high_frequency_terms(term_frequencies: Dict[str, int], config: ExtractionConfig) -> List[str]:
    """è¿‡æ»¤é«˜é¢‘ç‡æœ¯è¯­"""
    filtered_terms = []
    
    for term, frequency in term_frequencies.items():
        # é¢‘ç‡ç­›é€‰
        if frequency < config.min_term_frequency:
            continue
        
        filtered_terms.append(term)
        # # # è´¨é‡ç­›é€‰
        # if is_quality_term(term):
        #     filtered_terms.append(term)
    
    # æŒ‰é¢‘ç‡æ’åºï¼Œå–å‰50ä¸ª
    filtered_terms.sort(key=lambda x: term_frequencies[x], reverse=True)
    return filtered_terms[:50]

def is_quality_term(term: str) -> bool:
    """åˆ¤æ–­æ˜¯å¦ä¸ºé«˜è´¨é‡æœ¯è¯­"""
    term = term.lower().strip()
    
    # å¸¸è§è¯æ±‡é»‘åå•
    common_words = {
        'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
        'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before',
        'after', 'above', 'below', 'between', 'among', 'around', 'over',
        'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
        'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might',
        'can', 'must', 'shall', 'this', 'that', 'these', 'those', 'i', 'you',
        'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them',
        'my', 'your', 'his', 'her', 'its', 'our', 'their', 'very', 'so', 'too',
        'just', 'now', 'then', 'here', 'there', 'when', 'where', 'how', 'what',
        'who', 'which', 'why', 'if', 'because', 'since', 'until', 'while',
        'also', 'only', 'even', 'still', 'again', 'more', 'most', 'much',
        'many', 'some', 'any', 'all', 'both', 'each', 'every', 'other',
        'another', 'such', 'no', 'not', 'yes', 'get', 'make', 'take', 'come',
        'go', 'see', 'know', 'think', 'say', 'tell', 'feel', 'look', 'want',
        'use', 'find', 'give', 'work', 'call', 'try', 'ask', 'need', 'seem',
        'help', 'show', 'play', 'run', 'move', 'live', 'believe', 'hold',
        'bring', 'happen', 'write', 'provide', 'sit', 'stand', 'lose', 'pay'
    }
    
    if term in common_words:
        return False
    
    # é•¿åº¦æ£€æŸ¥
    if len(term) <= 2 or len(term) > 50:
        return False
    
    # çº¯æ•°å­—æˆ–ç‰¹æ®Šå­—ç¬¦
    if term.isdigit() or not re.search(r'[a-zA-Z]', term):
        return False
    
    # ä¸“ä¸šæœ¯è¯­ç‰¹å¾æ£€æµ‹
    professional_indicators = [
        r'[A-Z]{2,}',                    # å¤§å†™ç¼©å†™ (API, CPU)
        r'\w+tion$',                     # -tion ç»“å°¾
        r'\w+ness$',                     # -ness ç»“å°¾
        r'\w+ment$',                     # -ment ç»“å°¾
        r'\w+ology$',                    # -ology ç»“å°¾
        r'\w+system$',                   # system ç»“å°¾
        r'\w+technology$',               # technology ç»“å°¾
        r'\w+analysis$',                 # analysis ç»“å°¾
        r'\w+method$',                   # method ç»“å°¾
        r'\w+algorithm$',                # algorithm ç»“å°¾
        r'\w+protocol$',                 # protocol ç»“å°¾
        r'\w+framework$',                # framework ç»“å°¾
        r'\w+interface$',                # interface ç»“å°¾
        r'\w+network$',                  # network ç»“å°¾
        r'\w+database$',                 # database ç»“å°¾
    ]
    
    for pattern in professional_indicators:
        if re.search(pattern, term, re.IGNORECASE):
            return True
    
    # å¤åˆè¯æ£€æµ‹
    if '-' in term or '_' in term:
        return True
    
    # é©¼å³°å‘½åæ£€æµ‹
    if len(re.findall(r'[A-Z]', term)) >= 2:
        return True
    
    # é»˜è®¤ï¼šä¸­ç­‰é•¿åº¦çš„è¯æ±‡æœ‰å¯èƒ½æ˜¯æœ¯è¯­
    return 4 <= len(term) <= 20

def translate_terms_with_context(
    terms: List[str], 
    term_contexts: Dict[str, List[str]], 
    target_language: str,
    llm: Llama,
    stop_event: threading.Event,
    add_progress: Callable
) -> Dict[str, str]:
    """ğŸ”¥ å¸¦ä¸Šä¸‹æ–‡çš„æœ¯è¯­ç¿»è¯‘ï¼ˆä½¿ç”¨å†…éƒ¨ç¿»è¯‘å‡½æ•°ï¼‰"""
    global _progress_var
    glossary = {}
    batch_size = 10  
    progress_step = 0.3 / math.ceil(len(terms) / batch_size)
    batch_count = len(terms) // batch_size + 1
    
    for i in range(0, len(terms), batch_size):
        if stop_event.is_set():
            logger.info("æœ¯è¯­ç¿»è¯‘å·²è¢«å–æ¶ˆ")
            return {}
        
        batch_terms = terms[i:i + batch_size]
        batch_index = i // batch_size + 1
        
        try:
           
            logger.info(f"æ­£åœ¨ç¿»è¯‘ç¬¬ {batch_index} æ‰¹æœ¯è¯­ï¼ˆ{len(batch_terms)} ä¸ªï¼‰")

            # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„ç¿»è¯‘æç¤º
            prompt = build_context_aware_translation_prompt(
                batch_terms, term_contexts, target_language
            )
            
            # ğŸ”¥ ä½¿ç”¨å†…éƒ¨ç¿»è¯‘å‡½æ•°
            response = _create_chat_completion(prompt, llm)
            batch_glossary = parse_translation_response(response, batch_terms)
            
            # å»é‡ï¼šä»¥ç¬¬ä¸€æ¬¡ç¿»è¯‘ä¸ºå‡†
            for source, target in batch_glossary.items():
                if source not in glossary and target.strip():
                    glossary[source] = target.strip()

            add_progress(localization.get("log_glossary_batch_complete").format(batch_index=batch_index,batch_count=batch_count, chunk_count=batch_count), progress_step)

        except Exception as e:
            logger.error(f"ç¬¬ {batch_index} æ‰¹æœ¯è¯­ç¿»è¯‘å¤±è´¥: {e}")
            continue
    
    return glossary

def build_context_aware_translation_prompt(
    terms: List[str], 
    term_contexts: Dict[str, List[str]], 
    target_language: str
) -> str:
    """æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„ç¿»è¯‘æç¤º"""
    
    terms_with_context = []
    
    for i, term in enumerate(terms):
        contexts = term_contexts.get(term, [])
        
        # é€‰æ‹©æœ€æœ‰ä»£è¡¨æ€§çš„ä¸Šä¸‹æ–‡
        best_context = ""
        if contexts:
            # é€‰æ‹©æœ€é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆé€šå¸¸åŒ…å«æ›´å¤šä¿¡æ¯ï¼‰
            best_context = max(contexts, key=len)
            # æˆªæ–­è¿‡é•¿çš„ä¸Šä¸‹æ–‡
            if len(best_context) > 150:
                best_context = best_context[:150] + "..."
        
        term_info = f"{i+1}. **{term}**"
        if best_context:
            term_info += f"\n   ä¸Šä¸‹æ–‡: \"{best_context}\""
        
        terms_with_context.append(term_info)
    
    terms_text = "\n\n".join(terms_with_context)
    
    return f"""
è¯·å°†ä»¥ä¸‹æœ¯è¯­å‡†ç¡®ç¿»è¯‘æˆ{target_language}ã€‚æ¯ä¸ªæœ¯è¯­éƒ½æä¾›äº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œè¯·æ ¹æ®ä¸Šä¸‹æ–‡ç¡®å®šæœ€åˆé€‚çš„ç¿»è¯‘ã€‚

ã€ç¿»è¯‘è¦æ±‚ã€‘
1. æ ¹æ®ä¸Šä¸‹æ–‡ç†è§£æœ¯è¯­çš„å…·ä½“å«ä¹‰å’Œä½¿ç”¨åœºæ™¯
2. ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æ ‡å‡†è¯‘åï¼Œä¿æŒè¡Œä¸šä¸€è‡´æ€§
3. è€ƒè™‘æœ¯è¯­åœ¨ç‰¹å®šé¢†åŸŸçš„ä¸“ä¸šå«ä¹‰
4. ä¸“æœ‰åè¯å¯ä¿ç•™åŸæ–‡æˆ–ä½¿ç”¨é€šç”¨è¯‘æ³•
5. é¿å…è¿‡åº¦è§£é‡Šï¼Œä¿æŒè¯‘æ–‡ç®€æ´å‡†ç¡®

ã€æ³¨æ„äº‹é¡¹ã€‘
- åŒä¸€æœ¯è¯­åœ¨ä¸åŒä¸Šä¸‹æ–‡ä¸­å¯èƒ½æœ‰ä¸åŒç¿»è¯‘
- ä¼˜å…ˆä½¿ç”¨è¯¥é¢†åŸŸçš„æ ‡å‡†è¯‘å
- ä¿æŒè¯‘æ–‡çš„ä¸“ä¸šæ€§å’Œå¯è¯»æ€§

ã€è¾“å‡ºæ ¼å¼ã€‘
è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼š
{{
  "artificial intelligence": "äººå·¥æ™ºèƒ½",
  "machine learning": "æœºå™¨å­¦ä¹ ",
  "deep learning": "æ·±åº¦å­¦ä¹ "
}}

ã€å¾…ç¿»è¯‘æœ¯è¯­åŠä¸Šä¸‹æ–‡ã€‘
{terms_text}

è¯·å¼€å§‹ç¿»è¯‘ï¼š
"""

def parse_translation_response(response: str, original_terms: List[str]) -> Dict[str, str]:
    """è§£æç¿»è¯‘å“åº”"""
    try:
        # æå–JSONéƒ¨åˆ†
        json_match = re.search(r'\{.*?\}', response, re.DOTALL)
        if json_match:
            json_text = json_match.group()
            translations = json.loads(json_text)
            
            # éªŒè¯ç¿»è¯‘ç»“æœ
            valid_translations = {}
            original_terms_lower = [t.lower() for t in original_terms]
            
            for source, target in translations.items():
                source_clean = source.strip().lower()
                target_clean = str(target).strip()
                
                if source_clean in original_terms_lower and target_clean:
                    valid_translations[source_clean] = target_clean
            
            return valid_translations
        
        # å¤‡ç”¨è§£æ
        return fallback_parse_translation(response, original_terms)
        
    except Exception as e:
        logger.error(f"è§£æç¿»è¯‘å“åº”å¤±è´¥: {e}")
        return fallback_parse_translation(response, original_terms)

def fallback_parse_translation(response: str, original_terms: List[str]) -> Dict[str, str]:
    """å¤‡ç”¨ç¿»è¯‘è§£æ"""
    translations = {}
    lines = response.split('\n')
    original_terms_lower = [t.lower() for t in original_terms]
    
    for line in lines:
        line = line.strip()
        if ':' in line:
            parts = line.split(':', 1)
            if len(parts) == 2:
                source = parts[0].strip().strip('"').lower()
                target = parts[1].strip().strip('"')
                
                if source in original_terms_lower and target:
                    translations[source] = target
    
    return translations